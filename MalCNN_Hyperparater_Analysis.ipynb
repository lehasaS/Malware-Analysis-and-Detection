{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z-oHaidjKBs"
      },
      "source": [
        "# Training Malware Convolutional Network Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBh5pYJYjIQe"
      },
      "outputs": [],
      "source": [
        "%pip install wandb -Uq #on colab use !pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8PEOOsihnyB"
      },
      "source": [
        "The purpose of this notebook is to collect hyperparameters that provide the lowest loss given training and testing data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IknIepnaxCa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms\n",
        "import wandb\n",
        "import pprint\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cdb9-PIOa1dX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckRHCRS9P4CA"
      },
      "source": [
        "# Training Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAl_jZtoidJh"
      },
      "source": [
        "Download the output director with the tests sets and initialise a path to it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEFk5FFEa46e"
      },
      "outputs": [],
      "source": [
        "data_dir= pathlib.Path(os.path.normpath('/content/drive/MyDrive/output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBHn33M9a7f2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MalCnnOne(nn.Module):\n",
        "    CNN_name = \"Model One\"\n",
        "    def __init__(self, dropout, image_size):\n",
        "        super(MalCnnOne, self).__init__()\n",
        "        self.dropout=dropout\n",
        "        self.image_size=image_size\n",
        "        #Convolutions\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        #Pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        #Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.image_size*self.image_size,512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 2)\n",
        "\n",
        "        #flattener\n",
        "        self.flattening = torch.nn.Flatten()\n",
        "        \n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        \n",
        "        x = self.flattening(x)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MalCnnTwo(nn.Module):\n",
        "    CNN_name = \"Model Two\"\n",
        "    def __init__(self, dropout, image_dim):\n",
        "        super(MalCnnTwo, self).__init__()\n",
        "        self.dropout=dropout\n",
        "        self.image_dim=image_dim\n",
        "        #Convolutions\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        #Pooling layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        #Fully connected layers\n",
        "        self.temp = int((self.image_dim*self.image_dim)/4)\n",
        "        self.fc1 = nn.Linear(self.temp,1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 2)\n",
        "        \n",
        "        #Flattening\n",
        "        self.flattener = torch.nn.Flatten()\n",
        "\n",
        "        #Dropout\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "\n",
        "        x=self.flattener(x)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "\n",
        "        x = F.log_softmax(self.fc4(x),dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hip29Kf4bW_B"
      },
      "outputs": [],
      "source": [
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3f2XNJobc8Y"
      },
      "outputs": [],
      "source": [
        "def get_image_data_loaders(image_dim, batch_size):\n",
        "    train_dir = data_dir/'train'\n",
        "    val_dir = data_dir/'val'\n",
        "    test_dir =data_dir/'test'\n",
        "\n",
        "    transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Grayscale(),\n",
        "            torchvision.transforms.Resize((image_dim,image_dim)),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize((0.5, ), (0.5, ))\n",
        "        ])\n",
        "\n",
        "    training_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
        "    test_dataset = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
        "    val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, num_workers=0)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezGjerffioFb"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd3toZakbeYP"
      },
      "outputs": [],
      "source": [
        "def train(config=None):\n",
        "        with wandb.init(config=config):\n",
        "                config = wandb.config\n",
        "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "                if(config.model_name == \"Model One\"):\n",
        "                        network = MalCnnOne(config.dropout, config.image_size).to(device)\n",
        "                else:\n",
        "                        network = MalCnnTwo(config.dropout, config.image_size).to(device)\n",
        "                \n",
        "                print(\"Training for model: {}\".format(network.CNN_name))\n",
        "                model_string=''.join(random.choices(string.ascii_uppercase + string.digits, k=7))\n",
        "                \n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                train_loader, valid_loader, test_loader= get_image_data_loaders(config.image_size, config.batch_size)\n",
        "\n",
        "                optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
        "\n",
        "                valid_loss_min = np.Inf # track change in validation loss\n",
        "                for epoch in range(config.epochs):\n",
        "                        train_total_loss = 0\n",
        "                        train_loss=0\n",
        "                        ###################\n",
        "                        # train the model #\n",
        "                        ###################\n",
        "                        network.train()\n",
        "                        for images, labels in train_loader:\n",
        "                                images, labels = images.to(device), labels.to(device)\n",
        "                                optimizer.zero_grad()\n",
        "                                output = network(images)\n",
        "                                \n",
        "                                loss = criterion(output, labels)\n",
        "                                loss.backward()\n",
        "\n",
        "                                optimizer.step()\n",
        "                                train_loss += loss.item()     \n",
        "                                \n",
        "                                \n",
        "                        ######################    \n",
        "                        # validate the model #\n",
        "                        ######################\n",
        "                        valid_total_loss = 0\n",
        "                        valid_loss = 0\n",
        "                        network.eval()\n",
        "                        for images, labels in valid_loader:\n",
        "                                images,labels = images.to(device),labels.to(device)\n",
        "                                output = network(images)\n",
        "                                loss = criterion(output, labels)\n",
        "                                \n",
        "                                valid_loss += loss.item()\n",
        "                        \n",
        "                        train_total_loss = train_loss/len(train_loader.dataset)\n",
        "                        valid_total_loss = valid_loss/len(valid_loader.dataset)\n",
        "                        wandb.log({\"loss\":train_total_loss,\"epoch\":epoch})\n",
        "                        wandb.watch(network)\n",
        "                        \n",
        "                        # print training/validation statistics \n",
        "                        print(\"Epoch:{}/{} \\t average training loss:{:.4f} average validation loss:{:.4f}\".format(epoch + 1, config.epochs,\n",
        "                                                                                             train_total_loss,\n",
        "                                                                                             valid_total_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9dicAKviv6T"
      },
      "source": [
        "Hyperparameter tracking is done using Weights and Biases, below is the configuration that was used for the experiments done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqjKlW5MbwFs"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "        sweep_config = {\n",
        "        'method': 'grid'\n",
        "        }\n",
        "\n",
        "        metric = {\n",
        "        'name': 'loss',\n",
        "        'goal': 'minimize'   \n",
        "        }\n",
        "        sweep_config['metric'] = metric\n",
        "\n",
        "        parameters_dict = {\n",
        "        'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "        },\n",
        "        'batch_size': {\n",
        "        'value': 128\n",
        "        },\n",
        "        'learning_rate': {\n",
        "                'value' : 0.0001\n",
        "         },\n",
        "        'image_size': {\n",
        "        'values': [64,128,256]\n",
        "        },\n",
        "        'dropout': {\n",
        "          'value': 0.5\n",
        "        },\n",
        "        'epochs': {\n",
        "                'values': [20,30,40]\n",
        "        },\n",
        "        'model_name': {\n",
        "                'values' : [\"Model One\", \"Model Two\"]\n",
        "        }\n",
        "        }\n",
        "        sweep_config['parameters'] = parameters_dict\n",
        "        \n",
        "\n",
        "        sweep_id = wandb.sweep(sweep_config, project=\"CNN_Malware_Detection\")\n",
        "        pprint.pprint(sweep_config)\n",
        "\n",
        "        wandb.agent(sweep_id, train)\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEHnuTHab9Qy"
      },
      "outputs": [],
      "source": [
        "if __name__==\"__main__\":\n",
        "        main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL9vlwVB8Sq2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
